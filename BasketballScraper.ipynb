{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMaXMthe9N7GzHyI72qvzVc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ezesalvatore/205Lab/blob/main/BasketballScraper.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BasketBall Reference Uniform Scraper"
      ],
      "metadata": {
        "id": "x74FANU9a1s_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Project overview\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "MrkFzG-ptG9g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Objectives\n",
        "- Extract players name, team and uniform number\n",
        "- Support wireframe development\n",
        "- Shows my webscraping expertises\n",
        "\n",
        "### Methods used\n",
        "- The webscraper being used is Beautiful Soup, since Basketball Reference is primarily build with HTML\n",
        "- This project also make sure that it follows Basketball Reference rate limiting and crawl-delay violations"
      ],
      "metadata": {
        "id": "PFbJwK4Xy8Bw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "snKzXEmfyrK6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Makes a request to Basketball Reference\n",
        "import requests\n",
        "\n",
        "#How we are going to webscrape, able to parse through HTML to get the name, team, and uniform\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "#Helps me with exporting a csv file\n",
        "import pandas as pd\n",
        "\n",
        "#Allows me to add delays\n",
        "import time\n",
        "\n",
        "#Data processing and cleaning\n",
        "import re\n",
        "\n",
        "#Creates url for web scraping\n",
        "from urllib.parse import urljoin\n",
        "\n",
        "#Get rid of erros\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"Libraries imported\")"
      ],
      "metadata": {
        "id": "ykeBZbWRy3SI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Session Setup"
      ],
      "metadata": {
        "id": "G7D5IX6C42yK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def setup_session():\n",
        "    \"\"\"\n",
        "    This function is setting up the sessions for web scraping. Making sure the headers are compliant.\n",
        "\n",
        "    \"\"\"\n",
        "    session = requests.Session()\n",
        "\n",
        "    session.headers.update({\n",
        "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',\n",
        "        'Accept': 'text/html,application/xhtml+xml',\n",
        "        'Connection': 'keep-alive',\n",
        "    })\n",
        "\n",
        "    print(\"Session is created!\")\n",
        "\n",
        "    return session"
      ],
      "metadata": {
        "id": "HQSVR8cP46jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fetching Basketball Reference Data"
      ],
      "metadata": {
        "id": "Y7qOsl9f7f8V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fetch_basketball_data():\n",
        "    \"\"\"\n",
        "    Make single compliant request to Basketball Reference and following the robots.txt rules.\n",
        "\n",
        "    \"\"\"\n",
        "    url = \"https://www.basketball-reference.com/leagues/NBA_2025_numbers.html\"\n",
        "\n",
        "    session = setup_session()\n",
        "\n",
        "\n",
        "    try:\n",
        "        # Robots.txt compliance: Crawl-delay: 3\n",
        "        time.sleep(3.0)\n",
        "\n",
        "        response = session.get(url, timeout=30)\n",
        "\n",
        "        print(f\"The request to: {url} has been sent out\")\n",
        "\n",
        "        if response.status_code == 429:\n",
        "            raise Exception(\"Rate limited - blocked for 24 hours\")\n",
        "\n",
        "        response.raise_for_status()\n",
        "\n",
        "        print(f\"Server Status: {response.status_code}\")\n",
        "\n",
        "        #HTML page returned\n",
        "        return response.text\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\" Request failed: {e}\")\n",
        "        return None\n",
        "    finally:\n",
        "        session.close()"
      ],
      "metadata": {
        "id": "qrXYEOj270k8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Web Scraping"
      ],
      "metadata": {
        "id": "FV-FwvHQ2sNe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Web Scraping Strategy\n",
        "\n",
        "I will use the the CSS Selector to get the values from the html site\n",
        "\n",
        "**Uniform Number** : `div.data_grid_box table caption`\n",
        "<br>\n",
        "\n",
        "**Player Name**: `div.data_grid_box a[href*='/players/']`\n",
        "<br>\n",
        "\n",
        "**Team**: `span.desc a[href*='/teams/'] `\n",
        "\n"
      ],
      "metadata": {
        "id": "jTxlsmap26KI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_uniform_data(html_content):\n",
        "    \"\"\"\n",
        "    Parse HTML - Single row per player, highest GP team priority\n",
        "    \"\"\"\n",
        "    if not html_content:\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    soup = BeautifulSoup(html_content, 'html.parser')\n",
        "\n",
        "    # First pass: collect all player-team-number combinations\n",
        "    player_teams = {}\n",
        "    uniform_sections = soup.find_all('div', class_='data_grid_box')\n",
        "\n",
        "    print(f\"üîç Found {len(uniform_sections)} uniform number sections\")\n",
        "\n",
        "    for section in uniform_sections:\n",
        "        try:\n",
        "            caption = section.find('caption', class_='poptip')\n",
        "            if not caption:\n",
        "                continue\n",
        "\n",
        "            uniform_number = caption.get_text().strip()\n",
        "            player_rows = section.find_all('tr')\n",
        "\n",
        "            for row in player_rows:\n",
        "                player_link = row.find('a', href=lambda x: x and '/players/' in x)\n",
        "                if not player_link:\n",
        "                    continue\n",
        "\n",
        "                player_name = player_link.get_text().strip()\n",
        "\n",
        "                # Extract teams\n",
        "                team_span = row.find('span', class_='desc')\n",
        "                teams = []\n",
        "\n",
        "                if team_span:\n",
        "                    team_links = team_span.find_all('a', href=lambda x: x and '/teams/' in x)\n",
        "                    teams = [link.get_text().strip() for link in team_links]\n",
        "\n",
        "                # Store each team-number combination\n",
        "                if player_name not in player_teams:\n",
        "                    player_teams[player_name] = {}\n",
        "\n",
        "                for team in teams:\n",
        "                    player_teams[player_name][team] = uniform_number\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Error processing section: {e}\")\n",
        "            continue\n",
        "\n",
        "    print(f\"‚úÖ Collected data for {len(player_teams)} unique players\")\n",
        "    return player_teams"
      ],
      "metadata": {
        "id": "Zz0IZZF2Ch4i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Intergrate with other csv file"
      ],
      "metadata": {
        "id": "4E-m7Pc7n7ET"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_uniform_csv(player_teams):\n",
        "    \"\"\"Convert player uniform data to CSV format\"\"\"\n",
        "\n",
        "    players_data = []\n",
        "\n",
        "    for player_name, teams_numbers in player_teams.items():\n",
        "\n",
        "        if len(teams_numbers) == 1:\n",
        "            # Single team player\n",
        "            team = list(teams_numbers.keys())[0]\n",
        "            uniform_number = teams_numbers[team]\n",
        "\n",
        "            players_data.append({\n",
        "                'player_name': player_name,\n",
        "                'uniform_numbers': uniform_number,\n",
        "                'is_multi_team': False,\n",
        "                'all_teams': team,\n",
        "                'team_count': 1\n",
        "            })\n",
        "\n",
        "        else:\n",
        "            # Multi-team player - create \"17-71\" format\n",
        "            unique_numbers = list(set(teams_numbers.values()))\n",
        "            uniform_numbers_string = '-'.join(unique_numbers)\n",
        "\n",
        "            players_data.append({\n",
        "                'player_name': player_name,\n",
        "                'uniform_numbers': uniform_numbers_string,\n",
        "                'is_multi_team': True,\n",
        "                'all_teams': '-'.join(teams_numbers.keys()),\n",
        "                'team_count': len(teams_numbers)\n",
        "            })\n",
        "\n",
        "    return pd.DataFrame(players_data)"
      ],
      "metadata": {
        "id": "pmOYbS0boC2W"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}